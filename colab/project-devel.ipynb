{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copia de Project 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJbYXou6chZf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28c6f77c-eeaa-479f-c577-f3da0331c57c"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Apr 17 11:32:16 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   76C    P8    12W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUElMiSlyoMu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b5c9cd-9d17-41ee-8adf-0f7f1a2ef6e6"
      },
      "source": [
        "## Mount grdive unit in order to load data and import source files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mf5PxKCN_1kA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3452ce72-040b-417e-9fc0-7768e3d78337"
      },
      "source": [
        "!ls /content/gdrive/MyDrive/ColabNotebooks/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Copia de Project 2.ipynb'   microwave_out      redd.yaml\n",
            " dataset.py\t\t     modelclassatt.py   run.sh\n",
            " dataset_test.py\t     model.py\t        settings.yaml\n",
            " dishwasher_out\t\t    'Project 2.ipynb'   template.yaml\n",
            " fridge_out\t\t    'Project antic'     utils.py\n",
            " __init__.py\t\t     __pycache__\n",
            " main.py\t\t     redd.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5JgFZET__SG"
      },
      "source": [
        "## Include ColabNotebooks to syspath to let python load libraries\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/MyDrive/ColabNotebooks')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGzJnN_OACQY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d2a50fb-1eee-451c-916b-320d8ed1c61b"
      },
      "source": [
        "!ls /content/gdrive/MyDrive/datasetKorea/redd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "redd_house1_0.csv   redd_house1_8.csv  redd_house3_0.csv  redd_house4_3.csv\n",
            "redd_house1_10.csv  redd_house1_9.csv  redd_house3_1.csv  redd_house4_4.csv\n",
            "redd_house1_1.csv   redd_house2_0.csv  redd_house3_2.csv  redd_house4_5.csv\n",
            "redd_house1_2.csv   redd_house2_1.csv  redd_house3_3.csv  redd_house5_0.csv\n",
            "redd_house1_3.csv   redd_house2_2.csv  redd_house3_4.csv  redd_house6_0.csv\n",
            "redd_house1_4.csv   redd_house2_3.csv  redd_house3_5.csv  redd_house6_1.csv\n",
            "redd_house1_5.csv   redd_house2_4.csv  redd_house4_0.csv  redd_house6_2.csv\n",
            "redd_house1_6.csv   redd_house2_5.csv  redd_house4_1.csv  redd_house6_3.csv\n",
            "redd_house1_7.csv   redd_house2_6.csv  redd_house4_2.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-Lw8xnNACYG",
        "outputId": "fa35b159-c08f-43e8-98b3-022aa82ec596"
      },
      "source": [
        "!pip install ray"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ray in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from ray) (3.12.4)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray) (1.0.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from ray) (0.4.4)\n",
            "Requirement already satisfied: redis>=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ray) (3.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray) (3.0.12)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from ray) (0.10.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray) (2.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from ray) (3.7.4.post0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray) (7.1.2)\n",
            "Requirement already satisfied: gpustat in /usr/local/lib/python3.7/dist-packages (from ray) (0.6.0)\n",
            "Requirement already satisfied: opencensus in /usr/local/lib/python3.7/dist-packages (from ray) (0.7.12)\n",
            "Requirement already satisfied: aioredis in /usr/local/lib/python3.7/dist-packages (from ray) (1.3.1)\n",
            "Requirement already satisfied: py-spy>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ray) (0.3.5)\n",
            "Requirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from ray) (1.32.0)\n",
            "Requirement already satisfied: colorful in /usr/local/lib/python3.7/dist-packages (from ray) (0.5.4)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray) (1.19.5)\n",
            "Requirement already satisfied: aiohttp-cors in /usr/local/lib/python3.7/dist-packages (from ray) (0.7.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray) (3.13)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->ray) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->ray) (54.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (1.24.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->ray) (1.6.3)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->ray) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->ray) (3.7.4.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->ray) (5.1.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->ray) (20.3.0)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.7/dist-packages (from gpustat->ray) (7.352.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from gpustat->ray) (5.4.8)\n",
            "Requirement already satisfied: blessings>=1.6 in /usr/local/lib/python3.7/dist-packages (from gpustat->ray) (1.7)\n",
            "Requirement already satisfied: opencensus-context==0.1.2 in /usr/local/lib/python3.7/dist-packages (from opencensus->ray) (0.1.2)\n",
            "Requirement already satisfied: google-api-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from opencensus->ray) (1.26.3)\n",
            "Requirement already satisfied: hiredis in /usr/local/lib/python3.7/dist-packages (from aioredis->ray) (2.0.0)\n",
            "Requirement already satisfied: google-auth<2.0dev,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray) (1.28.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray) (2018.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray) (1.53.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray) (20.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray) (4.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2.0.0,>=1.0.0->opencensus->ray) (2.4.7)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSt-KQgAA0RR"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import os\n",
        "import sys\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from ray import tune\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from dataset import InMemoryKoreaDataset\n",
        "from model import ModelPaperBackward as Model\n",
        "from utils import error, load_yaml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrdjNfu1eD5B"
      },
      "source": [
        "####Single epoch (Train, Eval, Test)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGBkRyghd-Cb"
      },
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "def train_single_epoch(epoch, model, train_loader, optimizer, eval_loader,\n",
        "    plotfilename=None):\n",
        "    model.train()\n",
        "    errs, losses = [], []\n",
        "    x = torch.unsqueeze(x, dim=1)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    x, y, clas = x.to(device), y.to(device), clas.to(device)\n",
        "\n",
        "def eval_single_epoch(model, eval_loader, plotfilename=None):\n",
        "    errs, losses = [], []\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for idx, (x, y, clas) in enumerate(eval_loader):\n",
        "            x = torch.unsqueeze(x, dim=1)\n",
        "\n",
        "            x, y, clas = x.to(device), y.to(device), clas.to(device)\n",
        "            y_, reg_, alphas_, clas_ = model(x)\n",
        "\n",
        "            loss_clas = F.binary_cross_entropy(clas_, clas)\n",
        "            loss_out = F.mse_loss(y_, y)\n",
        "            loss = loss_out + loss_clas\n",
        "            err = error(y, y_)\n",
        "\n",
        "            loss_, err_ = loss.item(), err.item()\n",
        "            losses.append(loss_)\n",
        "            errs.append(err_)\n",
        "\n",
        "            if idx % 50 == 0:\n",
        "              print(f\"eval epoch={epoch} batch={idx+1} loss={loss:.2f} err={err:.2f}\")\n",
        "              if plotfilename:\n",
        "                  filename = plotfilename + f\".{idx}eval.png\"\n",
        "                  plot_window(\n",
        "                      x.cpu(),\n",
        "                      y.cpu(),\n",
        "                      y_.cpu(),\n",
        "                      reg_.cpu(),\n",
        "                      clas_.cpu(),\n",
        "                      alphas_.cpu(),\n",
        "                      loss_,\n",
        "                      err_,\n",
        "                      filename,\n",
        "                  )\n",
        "    return np.mean(losses), np.mean(errs)\n",
        "\n",
        "\n",
        "def test_single(model, test_loader, appliance, plotfilename=None):\n",
        "    errs, losses = [], []\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for idx, (x, y, clas) in enumerate(test_loader):\n",
        "            x = torch.unsqueeze(x, dim=1)\n",
        "\n",
        "            x, y, clas = x.to(device), y.to(device), clas.to(device)\n",
        "            y_, reg_, alphas_, clas_ = model(x)\n",
        "\n",
        "            loss_clas = F.binary_cross_entropy(clas_, clas)\n",
        "            loss_out = F.mse_loss(y_, y)\n",
        "            loss = loss_out + loss_clas\n",
        "            err = error(y, y_)\n",
        "\n",
        "            loss_, err_ = loss.item(), err.item()\n",
        "            losses.append(loss_)\n",
        "            errs.append(err_)\n",
        "\n",
        "            if idx % 500 == 0:\n",
        "                print(f\"eval batch={idx+1} loss={loss:.2f} err={err:.2f}\")\n",
        "                if plotfilename:\n",
        "                    filename = plotfilename + f\".{idx}.attention.png\"\n",
        "                    plot_window(\n",
        "                        x.cpu(),\n",
        "                        y.cpu(),\n",
        "                        y_.cpu(),\n",
        "                        reg_.cpu(),\n",
        "                        clas_.cpu(),\n",
        "                        alphas_.cpu(),\n",
        "                        loss_,\n",
        "                        err_,\n",
        "                        filename,\n",
        "                    )\n",
        "\n",
        "    return np.mean(losses), np.mean(errs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BU6kw9VqeKZu"
      },
      "source": [
        "###Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-HHDS8deRLK"
      },
      "source": [
        "def train_model(datapath, output, appliance, hparams, doplot=None, load=True):\n",
        "    buildings = appliance[\"buildings\"][\"train\"]\n",
        "    name = appliance[\"name\"]\n",
        "    params = appliance[\"hparams\"]\n",
        "    record_err = np.inf\n",
        "\n",
        "    active_threshold = appliance.get(\"active_threshold\", 0.15)\n",
        "    active_ratio = appliance.get(\"active_ratio\", 0.5)\n",
        "    active_oversample = appliance.get(\"active_oversample\", 2)\n",
        "\n",
        "    my_dataset = InMemoryKoreaDataset(\n",
        "        datapath,\n",
        "        buildings,\n",
        "        name,\n",
        "        windowsize=params[\"L\"],\n",
        "        active_threshold=active_threshold,\n",
        "        active_ratio=active_ratio,\n",
        "        active_oversample=active_oversample,\n",
        "    )\n",
        "\n",
        "    total_size = len(my_dataset)\n",
        "    train_size = int(hparams[\"train_size\"] * (total_size))\n",
        "    eval_size = total_size - train_size\n",
        "\n",
        "    print(\"============= DATASET =============\")\n",
        "    print(f\"Total size: {total_size}\".format(total_size))\n",
        "    print(f\"Train size: {train_size}\".format(train_size))\n",
        "    print(f\"Eval size: {eval_size}\".format(eval_size))\n",
        "    print(\"===================================\")\n",
        "\n",
        "    train_dataset, eval_dataset = torch.utils.data.random_split(\n",
        "        my_dataset, (train_size, eval_size)\n",
        "    )\n",
        "\n",
        "    filename = os.path.join(output, \"dataset.pt\")\n",
        "    save_dataset(train_dataset, eval_dataset, filename)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=hparams[\"batch_size\"], shuffle=True\n",
        "    )\n",
        "    eval_loader = torch.utils.data.DataLoader(\n",
        "        eval_dataset, batch_size=hparams[\"batch_size\"]\n",
        "    )\n",
        "\n",
        "    model = Model(params[\"L\"], params[\"F\"], params[\"K\"], params[\"H\"])\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), hparams[\"lr\"])\n",
        "    # scheduler = optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
        "\n",
        "    if load:\n",
        "        filename = os.path.join(output, appliance[\"filename\"])\n",
        "        record_err = load_model(filename, model, optimizer)\n",
        "\n",
        "    results = []\n",
        "    for epoch in range(hparams[\"epochs\"]):\n",
        "        filename = os.path.join(output, appliance[\"filename\"] + str(epoch))\n",
        "\n",
        "        plotfilename = None\n",
        "        if doplot:\n",
        "            plotfilename = filename\n",
        "\n",
        "        err_ = None\n",
        "        try:\n",
        "            train_single_epoch(\n",
        "                epoch, model, train_loader, optimizer, eval_loader, plotfilename)\n",
        "\n",
        "            loss_, err_ = eval_single_epoch(model, eval_loader)\n",
        "            print(\"==========================================\")\n",
        "            print(f\"eval loss={loss:.2f} err={err:.2f}\")\n",
        "            print(\"==========================================\")\n",
        "\n",
        "            # tune.report(eval_loss=loss_)\n",
        "            results.append([(epoch, loss, err), (epoch, loss_, err_)])\n",
        "\n",
        "            if err_ < record_err:\n",
        "                filename = os.path.join(output, appliance[\"filename\"])\n",
        "                save(model, optimizer, filename, err_)\n",
        "                record_err = err_\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "\n",
        "        # scheduler.step()\n",
        "    summary(output, results)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_model_wrapper(config):\n",
        "    datapath = config[\"datapath\"]\n",
        "    output = config[\"output\"]\n",
        "    appliance = config[\"appliance\"]\n",
        "    hparams = config[\"hparams\"]\n",
        "    doplot = config[\"doplot\"]\n",
        "    load = config[\"load\"]\n",
        "    tune_hparams = config[\"tune\"]\n",
        "\n",
        "    appliance[\"hparams\"][\"F\"] = tune_hparams[\"F\"]\n",
        "    appliance[\"hparams\"][\"K\"] = tune_hparams[\"K\"]\n",
        "    appliance[\"hparams\"][\"H\"] = tune_hparams[\"H\"]\n",
        "\n",
        "    return train_model(datapath, output, appliance, hparams, doplot, load)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZn2iwRCeSly"
      },
      "source": [
        "###Test model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlQkkLBZenJ3"
      },
      "source": [
        "def test_model(datapath, output, appliance, hparams, doplot=None):\n",
        "    buildings = appliance[\"buildings\"][\"test\"]\n",
        "    name = appliance[\"name\"]\n",
        "    params = appliance[\"hparams\"]\n",
        "\n",
        "    filename = os.path.join(output, appliance[\"filename\"])\n",
        "    plotfilename = None\n",
        "    if doplot:\n",
        "        plotfilename = filename\n",
        "\n",
        "    active_threshold = appliance.get(\"active_threshold\", 0.15)\n",
        "    active_ratio = appliance.get(\"active_ratio\", 0.5)\n",
        "    active_oversample = appliance.get(\"active_oversample\", 2)\n",
        "\n",
        "    my_dataset = InMemoryKoreaDataset(\n",
        "        datapath,\n",
        "        buildings,\n",
        "        name,\n",
        "        windowsize=params[\"L\"],\n",
        "        active_threshold=active_threshold,\n",
        "        active_ratio=active_ratio,\n",
        "        active_oversample=active_oversample,\n",
        "    )\n",
        "\n",
        "    my_dataset, _ = torch.utils.data.random_split(my_dataset, (len(my_dataset), 0))\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        my_dataset, batch_size=hparams[\"batch_size\"]\n",
        "    )\n",
        "\n",
        "    model = Model(params[\"L\"], params[\"F\"], params[\"K\"], params[\"H\"])\n",
        "    model = model.to(device)\n",
        "\n",
        "    name = appliance[\"name\"]\n",
        "    filename = os.path.join(output, appliance[\"filename\"])\n",
        "    load_model(filename, model)\n",
        "\n",
        "    output = os.path.join(output, f\"{name}\")\n",
        "    loss, err = test_single(model, test_loader, appliance, plotfilename)\n",
        "    print(f\"Test loss={loss:.2f} err={err:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSqH0LIkevF9"
      },
      "source": [
        "###Save/load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPLte57VevoJ"
      },
      "source": [
        "def save(model, optimizer, file_name_model, error):\n",
        "    print(\"Guardant...\")\n",
        "    torch.save(\n",
        "        {\n",
        "            \"error\": error,\n",
        "            \"model_state_dict\": model.state_dict(),\n",
        "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "        },\n",
        "        file_name_model,\n",
        "    )\n",
        "    print(\"Model guardat!\")\n",
        "\n",
        "\n",
        "def save_dataset(train_, test_, filename):\n",
        "    torch.save({\"train\": train_, \"test\": test_}, filename)\n",
        "\n",
        "\n",
        "def load_model(file_name_model, model, optimizer=None):\n",
        "    print(\"Loading model...\")\n",
        "    if torch.cuda.is_available():\n",
        "        state = torch.load(file_name_model)\n",
        "    else:\n",
        "        state = torch.load(file_name_model, map_location=torch.device(\"cpu\"))\n",
        "\n",
        "    model.load_state_dict(state[\"model_state_dict\"])\n",
        "    error = state[\"error\"]\n",
        "    print(\"Loaded model! Error rècord: {}\".format(error))\n",
        "    if optimizer:\n",
        "        optimizer.load_state_dict(state[\"optimizer_state_dict\"])\n",
        "    return error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tS6GF_m8e064"
      },
      "source": [
        "###Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "em_QoB9qACcH"
      },
      "source": [
        "def plot(x, y, yhat, loss, err, filename):\n",
        "    subplots = [221, 222, 223, 224]\n",
        "    plt.figure(1, figsize=(10, 8))\n",
        "    plt.subplots_adjust(top=0.88)\n",
        "    for i in range(4):\n",
        "        x_, y_, yhat_ = (\n",
        "            x.detach().numpy()[i][0],\n",
        "            y.detach().numpy()[i],\n",
        "            yhat.detach().numpy()[i],\n",
        "        )\n",
        "        plt.subplot(subplots[i])\n",
        "        plt.plot(range(len(x_)), x_, color=\"b\", label=\"x\")\n",
        "        plt.plot(range(len(y_)), y_, color=\"g\", label=\"y\")\n",
        "        plt.plot(range(len(yhat_)), yhat_, color=\"r\", label=\"yhat\")\n",
        "\n",
        "    plt.suptitle(f\"loss {loss:.2f} error {err:.2f}\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    plt.clf()\n",
        "\n",
        "\n",
        "# def plot_window(x, y, yhat, alphas, loss, err, filename):\n",
        "#    # Naive plot window\n",
        "#    subplt_x = 4\n",
        "#    subplt_y = 4\n",
        "#    plt.figure(1, figsize=(20, 16))\n",
        "#    plt.subplots_adjust(top=0.88)\n",
        "#\n",
        "#    idxs = np.random.randint(len(x), size=(subplt_x * subplt_y))\n",
        "#    for i, idx in enumerate(idxs):\n",
        "#        x_, y_, yhat_ = (\n",
        "#            x.detach().numpy()[idx][0],\n",
        "#            y.detach().numpy()[idx],\n",
        "#            yhat.detach().numpy()[idx],\n",
        "#        )\n",
        "#        alphas_ = alphas.detach().numpy()[idx].flatten()\n",
        "#        ax1 = plt.subplot(subplt_x, subplt_y, i + 1)\n",
        "#        ax2 = ax1.twinx()\n",
        "#        ax1.plot(range(len(x_)), x_, color=\"b\", label=\"x\")\n",
        "#        ax1.plot(range(len(y_)), y_, color=\"r\", label=\"y\")\n",
        "#        ax1.plot(range(len(yhat_)), yhat_, color=\"orange\", label=\"yhat\")\n",
        "#        ax2.fill_between(\n",
        "#            range(len(alphas_)), alphas_, alpha=0.5, color=\"lightgrey\", label=\"alpha\"\n",
        "#        )\n",
        "#\n",
        "#    plt.suptitle(f\"loss {loss:.2f} error {err:.2f}\")\n",
        "#    ax1.legend()\n",
        "#    ax2.legend()\n",
        "#    plt.legend()\n",
        "#    plt.tight_layout()\n",
        "#    plt.savefig(filename)\n",
        "#    plt.clf()\n",
        "\n",
        "\n",
        "def plot_window(x, y, yhat, reg, clas, alphas, loss, err, filename):\n",
        "    subplt_x = 4\n",
        "    subplt_y = 4\n",
        "    plt.figure(1, figsize=(20, 16))\n",
        "    plt.subplots_adjust(top=0.88)\n",
        "\n",
        "    idxs = np.random.randint(len(x), size=(subplt_x * subplt_y))\n",
        "    for i, idx in enumerate(idxs):\n",
        "        x_, y_, yhat_, reg_, clas_ = (\n",
        "            x.detach().numpy()[idx][0],\n",
        "            y.detach().numpy()[idx],\n",
        "            yhat.detach().numpy()[idx],\n",
        "            reg.detach().numpy()[idx],\n",
        "            clas.detach().numpy()[idx],\n",
        "        )\n",
        "        alphas_ = alphas.detach().numpy()[idx].flatten()\n",
        "        ax1 = plt.subplot(subplt_x, subplt_y, i + 1)\n",
        "        ax2 = ax1.twinx()\n",
        "        ax1.plot(range(len(x_)), x_, color=\"b\", label=\"x\")\n",
        "        ax1.plot(range(len(y_)), y_, color=\"r\", label=\"y\")\n",
        "        ax1.plot(range(len(reg_)), reg_, color=\"black\", label=\"reg\")\n",
        "        ax1.plot(range(len(yhat_)), yhat_, alpha=0.5, color=\"orange\", label=\"yhat\")\n",
        "        ax2.fill_between(\n",
        "            range(len(alphas_)), alphas_, alpha=0.5, color=\"lightgrey\", label=\"alpha\"\n",
        "        )\n",
        "        alphas_max = np.max(alphas_)\n",
        "        ax2.plot(\n",
        "            range(len(clas_)), clas_ * alphas_max, color=\"cyan\", alpha=0.25, label=\"reg\"\n",
        "        )\n",
        "\n",
        "    plt.suptitle(f\"loss {loss:.2f} error {err:.2f}\")\n",
        "    ax1.legend()\n",
        "    ax2.legend()\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    plt.clf()\n",
        "\n",
        "\n",
        "def summary(path, results):\n",
        "    df = pd.DataFrame(\n",
        "        [\n",
        "            {\n",
        "                \"epoch\": x[0][0],\n",
        "                \"train_loss\": x[0][1],\n",
        "                \"train_err\": x[0][2],\n",
        "                \"eval_loss\": x[1][1],\n",
        "                \"eval_err\": x[1][2],\n",
        "            }\n",
        "            for x in results\n",
        "        ]\n",
        "    ).set_index(\"epoch\")\n",
        "\n",
        "    columns = [\"train_loss\", \"eval_loss\"]\n",
        "    filename = os.path.join(path, \"results-loss.csv\")\n",
        "    df[columns].round(3).to_csv(filename, sep=\";\")\n",
        "    filename = os.path.join(path, \"results-loss.png\")\n",
        "\n",
        "    plt.figure(1, figsize=(10, 8))\n",
        "    df[columns].round(3).plot()\n",
        "    plt.savefig(filename)\n",
        "    plt.clf()\n",
        "\n",
        "    columns = [\"train_err\", \"eval_err\"]\n",
        "    filename = os.path.join(path, \"results-error.csv\")\n",
        "    df[columns].round(3).to_csv(filename, sep=\";\")\n",
        "    filename = os.path.join(path, \"results-error.png\")\n",
        "\n",
        "    plt.figure(1, figsize=(10, 8))\n",
        "    df[columns].round(3).plot()\n",
        "    plt.savefig(filename)\n",
        "    plt.clf()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jl__05W0e6lQ"
      },
      "source": [
        "###Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEbuoiWHACfL"
      },
      "source": [
        "def main(args):\n",
        "\n",
        "    if args[\"disable_random\"]:\n",
        "        torch.manual_seed(7)\n",
        "\n",
        "    train = args[\"train\"]\n",
        "    tune_enabled = args[\"tune\"]\n",
        "    output = args[\"path\"]\n",
        "    plot_disabled = args[\"disable_plot\"]\n",
        "\n",
        "    settings = load_yaml(args[\"settings\"])\n",
        "    appliance = args[\"appliance\"]\n",
        "\n",
        "    dataset = settings[\"dataset\"]\n",
        "    hparams = settings[\"hparams\"]\n",
        "    if args[\"epochs\"]:\n",
        "        hparams[\"epochs\"] = int(args[\"epochs\"])\n",
        "\n",
        "    appliance = settings[\"appliances\"][appliance]\n",
        "\n",
        "    datapath = dataset[\"path\"]\n",
        "    if train:\n",
        "        print(\"==========================================\")\n",
        "        print(f\"Training ONGOING\")\n",
        "        print(\"==========================================\")\n",
        "\n",
        "        if not tune_enabled:\n",
        "            my_model = train_model(\n",
        "                datapath,\n",
        "                output,\n",
        "                appliance,\n",
        "                hparams,\n",
        "                doplot=not plot_disabled,\n",
        "                load=True\n",
        "            )\n",
        "        else:\n",
        "            config = {\n",
        "                \"datapath\": datapath,\n",
        "                \"output\": output,\n",
        "                \"appliance\": appliance,\n",
        "                \"hparams\": hparams,\n",
        "                \"doplot\": not plot_disabled,\n",
        "                \"load\": False,\n",
        "                \"tune\": {\n",
        "                    \"F\": tune.grid_search([16, 32, 64]),\n",
        "                    \"K\": tune.grid_search([4, 8, 16]),\n",
        "                    \"H\": tune.grid_search([256, 512, 1024]),\n",
        "                },\n",
        "            }\n",
        "            analysis = tune.run(\n",
        "                train_model_wrapper,\n",
        "                metric=\"val_loss\",\n",
        "                mode=\"min\",\n",
        "                num_samples=5,\n",
        "                config=config,\n",
        "            )\n",
        "            print(\"==========================================\")\n",
        "            print(f\"Best hyperparameters\")\n",
        "            print(analysis.best_config)\n",
        "            print(\"==========================================\")\n",
        "\n",
        "        print(\"==========================================\")\n",
        "        print(f\"Training DONE\")\n",
        "        print(\"==========================================\")\n",
        "    else:\n",
        "        print(\"==========================================\")\n",
        "        print(f\"Testing ONGOING\")\n",
        "        print(\"==========================================\")\n",
        "        test_model(datapath, output, appliance, hparams, doplot=not plot_disabled)\n",
        "        print(\"==========================================\")\n",
        "        print(f\"Testing DONE\")\n",
        "        print(\"==========================================\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-Hn1v_tAfzc",
        "outputId": "ea1ff142-db1f-4ee6-d1b1-fd973330e8a9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7usI-cKSHV6o"
      },
      "source": [
        "### Args i execució"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTC0PxuvACjM"
      },
      "source": [
        "args = {\"settings\": \"/content/gdrive/MyDrive/ColabNotebooks/settings.yaml\", \"appliance\": \"microwave\",\n",
        "        \"path\": \"/content/gdrive/MyDrive/ColabNotebooks/microwave_out\", \"train\": True, \"epochs\": 1,\n",
        "        \"disable_random\": True, \"tune\": False, \"disable_plot\": False}\n",
        "\n",
        "main(args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2350M3EyQBQE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}